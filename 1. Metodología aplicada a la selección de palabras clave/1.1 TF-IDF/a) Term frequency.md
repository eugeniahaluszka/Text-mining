## Term frequency-inverse document frequency Tf-idf

En una primera instancia se realizó la selección basándose en Term frequency-inverse document frequency (Tf-idf). Se trata de una técnica que mide con qué frecuencia aparece un término o frase dentro de un documento determinado, y lo compara con el número de documentos que mencionan ese término dentro de una colección de documentos. Asimismo, en la segunda parte de la denominación de estas siglas (idf) hablamos de frecuencia inversa de documento, donde se disminuye el peso de los términos que son muy frecuentes en varios documentos que se estén teniendo en cuenta. De esta forma, se pueden determinar cuáles son los términos más frecuentes y exclusivos en un documento en particular. 
Es un método fiable para estimar la relevancia de un documento para un término. Se obtiene un TF-IDF alto mientras mayor es la frecuencia de un término en una documento particular, y menor es el número de aparición de ese término en otros documentos.

Según aumenta el número de documentos que incluyen el término, baja el valor de TF IDF, hasta el extremo de que puede llegar a 0, si todos o casi todos los documentos de una muestra amplia lo mencionan. Esto es algo que incorpora Safecont en sus cálculos, como veremos luego.
## Breve resumen del trabajo realizado en la notebook
Para aplicarlo a la selección de keywords se decidió generar una base de datos con tweets provenientes de usuarios de Argentina, mientras que por otro lado ya se contaba de manera previa con un dataset que había recolectado tweets filtrando con palabras clave relacionadas a la temática de obesidad y que fueron seleccionadas a priori. 
Para llevar a cabo esta metodología primero se eliminaron todos aquellos caracteres que generan ruido en los textos como son los emojis, las etiquetas HTML, las menciones, los hash-tags, los URLs, números, entre otros. Luego se tokenizaron las palabras de los tweets, esto consiste en dividir el texto en las unidades que lo componen, es decir, palabras. Después, cada palabra fue contada y ubicada en una tabla de doble entrada según el dataset de pertenencia. Aquellas ubicadas con mayor ocurrencia de término son las que pueden estar más altamente relacionadas con el tópico. Sin embargo, pueden aparecer términos que no están estrechamente relacionados como lo son las frases comunes utilizadas en un lenguaje.
Se contaron la cantidad de palabras, sin repetir, utilizadas en cada dataset y a su vez, la cantidad de palabras distintas usadas por cada tema. Después se procedió a eliminar las stopwords, algo que anteriormente no se había realizado. Las mismas se denominan en español “palabras vacias” y reciben ese nombre ya que no presentan significado como pueden ser artículos, pronombres, preposiciones, entre otras. Una vez eliminadas se determinaron las 10 palabras más frecuentes por dataset. 
