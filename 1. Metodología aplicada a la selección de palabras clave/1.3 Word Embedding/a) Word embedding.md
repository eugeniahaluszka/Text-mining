## Word embedding

El método de Word Embedding (WE) consiste en representar palabras en un espacio vectorial n-dimensional. Cada vector guarda información semántica, lo que permite que pueda ser asociado o disociado a otros vectores (palabras) según los distintos contextos gramaticales. De esta forma se tiene en cuenta tanto la semántica como las relaciones entre las palabras. 
Los métodos de WE aprenden una representación vectorial de valor real a partir de un corpus de texto. Este corpus es de gran importancia ya que le va a nutrir al WE con respecto a contextos gramaticales y usos de las palabras. Su objetivo es cuantificar y categorizar similitudes semánticas entre elementos lingüísticos. Es de esperar sinónimos y palabras intercambiables se encuentren cerca en ese espacio. Para ellos se define la distancia medida como euclídea, coseno, manhatan, etc. 

Además existen una gran variedad de métodos para generar WE y éstos métodos pueden aplicarse a distintos tipos de textos e idiomas. Así, cada conjunto de WE puede contener un conocimiento distinto a otros WE.

Es común que los investigadores pongan a su disposición de forma gratuita WE o incrustaciones de palabras previamente formadas, a menudo bajo una licencia de acceso abierto, para que puedan ser utilizarlas en otros proyectos. Existen dos opciones de uso de los embeddings pre-entrenados:
    -utilizarlos de manera estática, en donde el WE se mantiene y utiliza como componente de su modelo, siempre y cuando el WE se adapta al problema y da buenos resultados.
    -la otra forma es usarlo actualizado, donde el embedding pre-entrenado se utiliza para sembrar el modelo, pero el embedding se actualiza conjuntamente durante el entrenamiento del modelo. 

## Aplicaciones

    - Sistemas de traducción: Generalmente estos sistemas están formados por una red neuronal que actúa como codificador y una red neuronal que actúa como decodificador. Tanto la entrada como la salida de estas redes neuronales son secuencias de palabras, estas palabras se representan mediante word embeddings. 
    - Análisis de opinión en textos: Con el crecimiento de popularidad de las redes sociales, resulta muy interesante el desarrollo de un sistema capaz de analizar si las opiniones de sobre un producto son positivas o negativas. 
    - Generación de textos: Mediante el uso de redes neuronales recurrentes es posible generar texto de forma automática. Combinando estos modelos con redes convolucionales es incluso posible crear sistemas que anoten o describan imágenes.
    - Chatbot, o sistemas que respondan preguntas de usuarios. 

## Procedimiento realizado

Para la aplicación del WE se definieron cuáles serían las palabras semilla, aquellas que se utilizarían al inicio para consultar otras palabras que estén relacionadas y derivan de la palabra obesidad. Las palabras seleccionadas fueron: _obesidad, obeso, obesa, obesos y obesas_. Se utilizó un WE que ya había sido previamente entrenado con un corpus de tweets de Argentina (2018) y fue facilitado por docentes de la Diplomatura en Ciencias de Datos y Aprendizaje Automático (FAMAF-UNC). En este WE se utilizó la biblioteca FastText, la cual es una extensión de Word2vec. Se trata de una biblioteca liviana, gratuita y de código abierto que permite a los usuarios aprender representaciones de texto y clasificadores de texto. La misma sirve para el aprendizaje de incrustaciones de palabras y clasificación de texto. Una de las principales características es la utilización de la estructura interna de una palabra para mejorar las representaciones vectoriales obtenidas con el método de omisión de gramática. Esta biblioteca es la ideal para trabajar con textos recolectados de tweets, donde no se suelen respetar reglas gramaticales. 
Se trabajó sobre el WE consultando cada palabra semilla obteniendo las primeras 50 palabras, ordenadas según similitud coseno. Mientras más alta es la similitud, menor distancia existe entre esos vectores (palabras) en el espacio, y por ende más relación guardan entre ellos. De las palabras obtenidas, se seleccionaron metodológicamente sólo aquellas que cumplieran con un criterio de sinonimia, es decir, que pudieran ser utilizadas en reemplazo de la palabra semilla, respetando el significado semántico y reduciendo ambigüedades del término. 
En el archivo "b) Tesauro" se podrán observar estas 50 palabras obtenidas por cada palabra semilla definida. A su vez se muestra en esta tabla el valor de similitud coseno, una breve descripción de la palabra y un contexto para esa palabra. La descripción fue construída a partir de la definición propuesta por la RAE para esa palabra. Aquellas que no tenían una definición asignada en la RAE se dejó el espacio en blanco que será completado a partir de otras fuentes como diccionarios de lunfardo, regionalismos y modismos. El contexto de cada palabra fue dado por la búsqueda de las mismas en tweets de la base de datos presentada en las dos notebooks anteriores (b-TF notebook y b-Probability of words notebook). De esta forma se pudo caracterizar cada una de las palabras obtenidas y junto con el contexto asignado por los tweets se decidió que palabras serían seleccionadas como _keywords_, teniendo en cuenta el conocimiento a priori de los expertos en el área. Las _keywords_ seleccionadas fueron pintadas de verde en el excel adjunto como archvio b) tesauro.


## Conclusión 

A partir de la lectura y conocimiento adquirido sobre los word embeddings, es que la consideramos como la técnica adecuada y elegida para conseguir nuestro objetivo. Las ventajas de la misma han sido mencionadas anteriormente. Si bien este WE no es de nuestra autoría cabe resaltar que es igualmente válido para cumplir los objetivos planteados. 
